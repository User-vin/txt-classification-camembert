{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCamembertModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFCamembertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFCamembertModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Début entraînement ss_cnn\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_camembert_model_2/roberta/pooler/dense/kernel:0', 'tf_camembert_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_camembert_model_2/roberta/pooler/dense/kernel:0', 'tf_camembert_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7928 - accuracy: 0.5033\n",
      "Epoch 1: val_loss improved from inf to 0.79105, saving model to ../results/ss_cnn\\ss_cnn_best_model.h5\n",
      "10/10 [==============================] - 27s 2s/step - loss: 0.7928 - accuracy: 0.5033 - val_loss: 0.7911 - val_accuracy: 0.4655\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7403 - accuracy: 0.5298\n",
      "Epoch 2: val_loss improved from 0.79105 to 0.70701, saving model to ../results/ss_cnn\\ss_cnn_best_model.h5\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.7403 - accuracy: 0.5298 - val_loss: 0.7070 - val_accuracy: 0.4982\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.6424\n",
      "Epoch 3: val_loss did not improve from 0.70701\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "10/10 [==============================] - 9s 951ms/step - loss: 0.6668 - accuracy: 0.6424 - val_loss: 0.7119 - val_accuracy: 0.5200\n",
      "Epoch 3: early stopping\n",
      "\\Fin entraînement ss_cnn\n",
      "\n",
      "7/7 [==============================] - 3s 352ms/step - loss: 0.7137 - accuracy: 0.4950\n",
      "\n",
      "Evaluation...\n",
      "\n",
      "{'loss': 0.7137081623077393, 'accuracy': 0.49504950642585754}\n",
      "\n",
      "Prédictions...\n",
      "\n",
      "7/7 [==============================] - 4s 339ms/step\n",
      "\n",
      "Sexe - Accuracy: 0.49504950495049505 \n",
      "Precision: 0.5052631578947369 \n",
      "Recall: 0.46601941747572817 \n",
      "F1 Score: 0.48484848484848486 \n",
      "AUC: 0.49563597136412674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import data\n",
    "import os\n",
    "\n",
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    # Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling1D,\n",
    ")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "\n",
    "from training_functions import (\n",
    "    # calculate_estimated_year_tensor,\n",
    "    custom_loss,\n",
    "    # custom_objects_dict,\n",
    "    custom_metric,\n",
    "    evaluate_model,\n",
    "    map_true_date_to_interval,\n",
    "    prediction,\n",
    "    save_accuracy_by_interval_and_gender,\n",
    "    save_hist_confusion_matrix,\n",
    "    save_training_history,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def ss_cnn(\n",
    "    model_id: str=None,\n",
    "    max_length: int=514,\n",
    "    dense_units: int=16,\n",
    "    conv_filters: int=32, \n",
    "    conv_kernel_size: int=3, \n",
    "):\n",
    "    \"\"\"\n",
    "    Crée un modèle Keras avec un modèle BERT pré-entraîné pour une tâche de classification du sexe.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: str, identifiant du modèle pré-entraîné à utiliser (par exemple, 'bert-base-uncased')\n",
    "    - max_length: int, la longueur maxiHomme des séquences d'entrée\n",
    "    - dense_units: int, nombre d'unités pour les couches denses\n",
    "    - conv_filters: int, nombre de filtres pour la couche Conv1D\n",
    "    - conv_kernel_size: int, taille du noyau pour la couche Conv1D\n",
    "\n",
    "    Returns:\n",
    "    - model: Keras Model, le modèle compilé\n",
    "    \"\"\"\n",
    "    # Charger le modèle BERT pré-entraîné\n",
    "    bert_model = TFAutoModel.from_pretrained(model_id)\n",
    "\n",
    "    # Définir les entrées du modèle\n",
    "    input_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    # Passer les entrées dans le modèle BERT\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    sequence_output = bert_output.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "    # CNN\n",
    "    conv_layer = Conv1D(filters=conv_filters, kernel_size=conv_kernel_size, activation=\"relu\", name=\"Conv1D\")(sequence_output)\n",
    "    pooling_layer = MaxPooling1D(pool_size=2, name=\"MaxPooling1D\")(conv_layer)\n",
    "    flatten_layer = Flatten(name=\"Flatten\")(pooling_layer)\n",
    "    dropout_layer = Dropout(0.3, name=\"Dropout\")(flatten_layer)\n",
    "\n",
    "    # Dense layer for sex classification\n",
    "    dense_layer_sexe = Dense(units=dense_units, activation=\"relu\", name=\"Dense_sexe\")(dropout_layer)\n",
    "\n",
    "    # Output layer for sex classification\n",
    "    sexe_output = Dense(1, activation=\"sigmoid\", name=\"Sexe_output\")(dense_layer_sexe)\n",
    "\n",
    "    # Créer le modèle\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=sexe_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if True:\n",
    "    # 1 : Charger les données d'entraînements et de validation\n",
    "    inputs_and_labels = data.main()\n",
    "\n",
    "    train_inputs = inputs_and_labels[\"train_inputs\"]\n",
    "    train_sexe_labels = inputs_and_labels[\"train_sexe_labels\"]\n",
    "    # train_date_labels = inputs_and_labels[\"train_date_labels\"]\n",
    "\n",
    "    val_inputs = inputs_and_labels[\"val_inputs\"]\n",
    "    val_sexe_labels = inputs_and_labels[\"val_sexe_labels\"]\n",
    "    # val_date_labels = inputs_and_labels[\"val_date_labels\"]\n",
    "\n",
    "    # Libérer la mémoire occupée par inputs_and_labels\n",
    "    del inputs_and_labels\n",
    "\n",
    "    # Créer le répertoire s'il n'existe pas\n",
    "    os.makedirs(config.SS_CNN_RESULT_PATH, exist_ok=True)\n",
    "\n",
    "    # 1. Intialiser le modèle\n",
    "    model = ss_cnn(model_id=config.MODEL_ID)\n",
    "\n",
    "    # 2. Compiler le modèle avec les fonctions de perte appropriées pour chaque sortie\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            \"Sexe_output\": \"binary_crossentropy\", \n",
    "            # \"Date_output\": custom_loss,\n",
    "        },\n",
    "        loss_weights={\n",
    "            \"Sexe_output\": config.SEXE_LOSS_WEIGHT,\n",
    "            # \"Date_output\": config.DATE_LOSS_WEIGHT\n",
    "        },\n",
    "        metrics={\n",
    "            \"Sexe_output\": \"accuracy\", \n",
    "            # \"Date_output\": custom_metric,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Sauvegarder l'architecture du modèle en .png\n",
    "    plot_model(model=model, show_shapes=True, to_file=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_model_arch.png\")\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_best_model.h5\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        mode=\"min\", # Sauvegarder le modèle avec la perte minimale\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=config.PATIENCE,\n",
    "        verbose=1, # Affichage d'un message\n",
    "        restore_best_weights=True # Restaurer les poids du meilleur modèle après l'arrêt\n",
    "    )\n",
    "\n",
    "    # 3. Entraîner le modèle\n",
    "    print(\"\\nDébut entraînement ss_cnn\\n\")\n",
    "    history = model.fit(\n",
    "        x=train_inputs,\n",
    "        y={\n",
    "            \"Sexe_output\": train_sexe_labels, \n",
    "            # \"Date_output\": train_date_labels,\n",
    "            },\n",
    "        epochs=config.EPOCHS,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        callbacks=[\n",
    "            checkpoint_callback, \n",
    "            early_stopping_callback,\n",
    "        ],\n",
    "        validation_data=(\n",
    "            val_inputs, \n",
    "            {\n",
    "                \"Sexe_output\": val_sexe_labels, \n",
    "                # \"Date_output\": val_date_labels,\n",
    "            }),\n",
    "    )\n",
    "    print(\"\\Fin entraînement ss_cnn\\n\")\n",
    "        \n",
    "    # Sauvegarder l'historique de l'entraînement en .json\n",
    "    with open(config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_history.json\", \"w\") as json_file:\n",
    "        json.dump(history.history, json_file, indent=4)\n",
    "    # Sauvegarder l'historique de l'entraînement en .png\n",
    "    save_training_history(history_data=history.history, save_dir=config.SS_CNN_RESULT_PATH, base_filename=config.SS_CNN, single_figure=True)\n",
    "    save_training_history(history_data=history.history, save_dir=config.SS_CNN_RESULT_PATH, base_filename=config.SS_CNN, single_figure=False)\n",
    "\n",
    "\n",
    "    # Evaluation --------------------------------\n",
    "\n",
    "\n",
    "    # 1. Charger les données de tests\n",
    "    inputs_and_labels = data.main()\n",
    "\n",
    "    test_inputs = inputs_and_labels[\"test_inputs\"]\n",
    "    test_sexe_labels = inputs_and_labels[\"test_sexe_labels\"]\n",
    "    test_date_labels = inputs_and_labels[\"test_date_labels\"]\n",
    "\n",
    "    # Libérer la mémoire occupée par inputs_and_labels\n",
    "    del inputs_and_labels\n",
    "\n",
    "    # Évaluer le modèle sur les données de test avec le GPU\n",
    "    evaluation_results = model.evaluate(\n",
    "        x = test_inputs,\n",
    "        y = {\n",
    "            \"Sexe_output\": np.array(test_sexe_labels),\n",
    "            # \"Date_output\": np.array(test_date_labels),\n",
    "        },\n",
    "        return_dict=True\n",
    "    )\n",
    "        \n",
    "    print(\"\\nEvaluation...\\n\")\n",
    "    pprint.pp(evaluation_results)\n",
    "\n",
    "    # Sauvegarder les résultats de l'évaluation\n",
    "    with open(config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_evaluation_results.json\", \"w\") as json_file:\n",
    "        json.dump(evaluation_results, json_file, indent=4)\n",
    "        \n",
    "    # Charger le modèle pré entraîné\n",
    "    # model = load_model(\n",
    "    #     filepath=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_best_model.h5\", \n",
    "    #     custom_objects=custom_objects_dict()\n",
    "    # )\n",
    "\n",
    "    print(\"\\nPrédictions...\\n\")\n",
    "    prediction_df = prediction(model=model, input_data=test_inputs, sexe_label=test_sexe_labels, date_label=test_date_labels)\n",
    "\n",
    "    evaluate_model(\n",
    "        df=prediction_df, \n",
    "        confusion_matrix_output=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_confusion_matrix.png\", \n",
    "        roc_curve_output=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_roc_curve.png\",\n",
    "        json_output=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_predictions_metrics.json\"\n",
    "    )\n",
    "\n",
    "    # prediction_df[\"interval\"] = prediction_df[\"true date\"].map(config.DATE_MAP)\n",
    "    prediction_df[\"interval\"] = prediction_df[\"true date\"].apply(map_true_date_to_interval)\n",
    "    \n",
    "    save_hist_confusion_matrix(df=prediction_df, output_file=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_hist_confusion_matrix.png\")\n",
    "\n",
    "    save_accuracy_by_interval_and_gender(df=prediction_df, output_file=config.SS_CNN_RESULT_PATH + f\"{config.SS_CNN}_accuracy_by_interval_sexes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true sexe</th>\n",
       "      <th>pred sexe</th>\n",
       "      <th>true date</th>\n",
       "      <th>interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1827</td>\n",
       "      <td>[1825, 1850)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1831</td>\n",
       "      <td>[1825, 1850)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1832</td>\n",
       "      <td>[1825, 1850)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1832</td>\n",
       "      <td>[1825, 1850)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1832</td>\n",
       "      <td>[1825, 1850)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>[2000, 2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>[2000, 2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2000, 2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2000, 2024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2000, 2024)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     true sexe  pred sexe  true date      interval\n",
       "0            1          1       1827  [1825, 1850)\n",
       "1            1          0       1831  [1825, 1850)\n",
       "2            0          1       1832  [1825, 1850)\n",
       "3            0          0       1832  [1825, 1850)\n",
       "4            0          0       1832  [1825, 1850)\n",
       "..         ...        ...        ...           ...\n",
       "197          0          1       2017  [2000, 2024)\n",
       "198          0          0       2017  [2000, 2024)\n",
       "199          0          1       2022  [2000, 2024)\n",
       "200          0          0       2022  [2000, 2024)\n",
       "201          0          0       2022  [2000, 2024)\n",
       "\n",
       "[202 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCamembertModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFCamembertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFCamembertModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Début entraînement ss_cnn\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_camembert_model/roberta/pooler/dense/kernel:0', 'tf_camembert_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_camembert_model/roberta/pooler/dense/kernel:0', 'tf_camembert_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9592 - accuracy: 0.4901\n",
      "Epoch 1: val_loss improved from inf to 0.74454, saving model to ../results/ss_cnn\\ss_cnn_best_model.h5\n",
      "10/10 [==============================] - 21s 1s/step - loss: 0.9592 - accuracy: 0.4901 - val_loss: 0.7445 - val_accuracy: 0.4545\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7265 - accuracy: 0.5828\n",
      "Epoch 2: val_loss improved from 0.74454 to 0.72700, saving model to ../results/ss_cnn\\ss_cnn_best_model.h5\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.7265 - accuracy: 0.5828 - val_loss: 0.7270 - val_accuracy: 0.5309\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.5695\n",
      "Epoch 3: val_loss improved from 0.72700 to 0.72525, saving model to ../results/ss_cnn\\ss_cnn_best_model.h5\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.6533 - accuracy: 0.5695 - val_loss: 0.7253 - val_accuracy: 0.4873\n",
      "\\Fin entraînement ss_cnn\n",
      "\n",
      "7/7 [==============================] - 3s 344ms/step - loss: 0.7162 - accuracy: 0.5198\n",
      "\n",
      "Evaluation...\n",
      "\n",
      "{'loss': 0.7162123322486877, 'accuracy': 0.5198019742965698}\n",
      "\n",
      "Prédictions...\n",
      "\n",
      "7/7 [==============================] - 4s 343ms/step\n",
      "\n",
      "Sexe - Accuracy: 0.5198019801980198 \n",
      "Precision: 0.5277777777777778 \n",
      "Recall: 0.5533980582524272 \n",
      "F1 Score: 0.5402843601895734 \n",
      "AUC: 0.519123271550456\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 227\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m evaluate_model(\n\u001b[0;32m    220\u001b[0m     df\u001b[38;5;241m=\u001b[39mprediction_df, \n\u001b[0;32m    221\u001b[0m     confusion_matrix_output\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSS_CNN_RESULT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mSS_CNN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_confusion_matrix.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    222\u001b[0m     roc_curve_output\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSS_CNN_RESULT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mSS_CNN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_roc_curve.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m     json_output\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSS_CNN_RESULT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mSS_CNN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_predictions_metrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# prediction_df[\"interval\"] = prediction_df[\"true date\"].map(config.DATE_MAP)\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m prediction_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterval\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_true_date_to_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m save_hist_confusion_matrix(df\u001b[38;5;241m=\u001b[39mprediction_df, output_file\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSS_CNN_RESULT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mSS_CNN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hist_confusion_matrix.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m save_accuracy_by_interval_and_gender(df\u001b[38;5;241m=\u001b[39mprediction_df, output_file\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSS_CNN_RESULT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mSS_CNN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_accuracy_by_interval_sexes.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Documents\\Cours\\M2\\Modélisation de système de vision\\Projet\\projet-MSV\\scripts\\training_functions.py:154\u001b[0m, in \u001b[0;36mmap_true_date_to_interval\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mDATE_MAP\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    153\u001b[0m     start, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, value\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[]()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m date \u001b[38;5;241m<\u001b[39m end:\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
